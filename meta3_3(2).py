# -*- coding: utf-8 -*-
"""Meta3.3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CxB6WomhmzIyMcU15GTZRJBXRbqwYKIy
"""

from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments, LlamaForSequenceClassification
import torch
from torch.utils.data import DataLoader
import os
import subprocess
import json

# Download dataset from GitHub
repo_url = "https://github.com/ltgoslo/norec.git"
dataset_path = "./norec"
if not os.path.exists(dataset_path):
    subprocess.run(["git", "clone", repo_url, dataset_path])

import os
os.environ["NEBIUS_API_KEY"] = "eyJhbGciOiJIUzI1NiIsImtpZCI6IlV6SXJWd1h0dnprLVRvdzlLZWstc0M1akptWXBvX1VaVkxUZlpnMDRlOFUiLCJ0eXAiOiJKV1QifQ.eyJzdWIiOiJnb29nbGUtb2F1dGgyfDExODExNzk3MzQ3MzE2MzA4NzM1NyIsInNjb3BlIjoib3BlbmlkIG9mZmxpbmVfYWNjZXNzIiwiaXNzIjoiYXBpX2tleV9pc3N1ZXIiLCJhdWQiOlsiaHR0cHM6Ly9uZWJpdXMtaW5mZXJlbmNlLmV1LmF1dGgwLmNvbS9hcGkvdjIvIl0sImV4cCI6MTg5NjEwMTEyNiwidXVpZCI6IjJiY2ZlYmU0LTkyZjMtNGY3NC1iZTFlLWYwNTMxNzI0ZWYyMiIsIm5hbWUiOiJNZXRhIiwiZXhwaXJlc19hdCI6IjIwMzAtMDEtMzFUMTQ6NDU6MjYrMDAwMCJ9.HWBvay7K9c0XcPwTRBk3tX86BlBi6mH6K-YC18ohdKM"
#

import os
print(os.environ.get("NEBIUS_API_KEY"))

import os
from openai import OpenAI, OpenAIError

# Get API Key
api_key = os.environ.get("NEBIUS_API_KEY")
if not api_key:
    raise ValueError("NEBIUS_API_KEY not found! Set it as an environment variable.")

# Initialize Nebius AI client
client = OpenAI(
    base_url="https://api.studio.nebius.ai/v1/",
    api_key=api_key
)

try:
    # Make a request
    response = client.chat.completions.create(
        model="meta-llama/Llama-3.3-70B-Instruct",
        temperature=0,
        messages=[{"role": "system", "content": "what is 2+2?"}]
    )

    #print(response.to_json())
    print(response.choices[0].message.content)



except OpenAIError as e:
    print("ðŸš¨ API Error:", e)

### DO NOT TOUCH ANYTHING ABOVE THIS

!pip install sentencepiece

import os

data_folders = [os.path.join(dataset_path, "data/train")]

output_file = os.path.join(dataset_path, "norec_combined.txt")  # Save inside dataset_path

with open(output_file, "w", encoding="utf-8") as outfile:
    for folder in data_folders:
        if os.path.exists(folder):  # Ensure the folder exists
            for filename in os.listdir(folder):
                file_path = os.path.join(folder, filename)
                if filename.endswith(".txt"):
                    with open(file_path, "r", encoding="utf-8") as infile:
                        outfile.write(infile.read() + "\n")  # Add newline for separation

print(f"âœ… Merged dataset saved as {output_file}")

!pip install --upgrade sentencepiece

import sentencepiece as spm

import sentencepiece as spm

# Ensure the dataset file exists
data_file = os.path.join(dataset_path, "norec_combined.txt")
if not os.path.exists(data_file):
    raise FileNotFoundError(f"Dataset file not found: {data_file}")

# Train SentencePiece tokenizer
spm.SentencePieceTrainer.train(
    input=data_file,
    model_prefix="norec_tokenizer",
    vocab_size=32000,
    model_type="bpe",
    character_coverage=0.9995,
    max_sentence_length=4096
)

print("Tokenizer training complete! Model saved as 'norec_tokenizer.model'")

""""sp = spm.SentencePieceProcessor()
sp.load("norec_tokenizer.model")

# Example text in Norwegian
text = "Dette er en testsetning for den norske tokenizeren."

# Encode text to token IDs
tokens = sp.encode(text, out_type=int)
print("Token IDs:", tokens)

# Decode token IDs back to text
decoded_text = sp.decode(tokens)
print("Decoded Text:", decoded_text)
""""

print(dataset_path)

import os

def load_dataset(dataset_path):
    dataset = {"train": [], "test": [], "dev": []}

    for split in dataset.keys():
        split_path = os.path.join(dataset_path, "data", split)
        for filename in os.listdir(split_path):
            file_path = os.path.join(split_path, filename)
            if filename.endswith(".txt"):
                with open(file_path, "r", encoding="utf-8") as f:
                    text = f.read().strip()
                    dataset[split].append({"text": text})

    return dataset

dataset_path = "./norec"
dataset = load_dataset(dataset_path)

from huggingface_hub import login

login(token="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")

from transformers import AutoTokenizer

# Download once & reuse
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b", cache_dir="./cache")

def fast_tokenize(text):
    return tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=512)

# Set pad_token to eos_token
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(example):
    return tokenizer(
        example["text"],
        padding="max_length",
        truncation=True,
        max_length=1293,  # Ensure this matches your model's expected input size
        return_tensors="pt",  # Ensures PyTorch tensors
        return_attention_mask=True  # <-- Fix: Include attention mask
    )

tokenized_dataset = {
    split: [tokenize_function(example) for example in dataset[split]]
    for split in dataset
}

save_path = "./llama_finetuned_model"
from transformers import LlamaForCausalLM, AutoTokenizer

load_path = "./llama_finetuned_model"

# Load model and tokenizer
# Check if model exists
if os.path.exists(save_path):
    print("Loading existing model...")
    model_clm = LlamaForCausalLM.from_pretrained(save_path)
    tokenizer = AutoTokenizer.from_pretrained(save_path)

!pip install bitsandbytes

import torch
from transformers import LlamaForCausalLM, Trainer, TrainingArguments
"""# del model_clm  # Delete the existing model
torch.cuda.empty_cache()  # Clear GPU memory

model_clm = LlamaForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="cuda"
)"""
from transformers import BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  # Use 4-bit mode (or use load_in_8bit=True for 8-bit)
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    llm_int8_skip_modules=["lm_head"],  # Helps with stability
)

!pip install -U bitsandbytes

!pip install -U transformers

from transformers import LlamaForCausalLM, AutoTokenizer
import torch
from accelerate import load_checkpoint_and_dispatch

model_name = "meta-llama/Llama-3.3-70B-Instruct"

# Load model with automatic device placement (avoids full GPU loading)
model = LlamaForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,  # Use fp16 for speed
    device_map="auto"  # Distributes across available GPUs (or CPU if needed)
)

# Offload unused parts to CPU or disk
#model = load_checkpoint_and_dispatch(model, device_map="auto", offload_folder="offload")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("âœ… Model loaded efficiently!")

from google.colab import drive
drive.mount('/content/drive')

"""
model_name="meta-llama/Llama-3.3-70B-Instruct"

model = LlamaForCausalLM.from_pretrained(model_name) #quantization_config=bnb_config)
"""

!pip install peft

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8, lora_alpha=16, target_modules=["q_proj", "v_proj"], lora_dropout=0.05
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # Check how many parameters are trained

from transformers import TrainingArguments, Trainer

training_args_clm = TrainingArguments(
    output_dir="./llama_norwegian_clm",
    per_device_train_batch_size=8,  # LoRA allows a larger batch size
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=16,  # Helps simulate larger batch size
    num_train_epochs=3,
    save_steps=500,
    save_total_limit=2,
    evaluation_strategy="steps",
    eval_steps=500,
    logging_dir="./logs_clm",
    logging_steps=100,
    fp16=True,  # Still useful for speed on most GPUs
    optim="adamw_torch",  # Standard optimizer works fine with LoRA
    report_to="none",
    gradient_checkpointing=False,  # Not needed for LoRA (memory efficient already)
)

# Explicitly set `pad_token_id` in Trainer
model_clm.config.pad_token_id = tokenizer.pad_token_id

trainer_clm = Trainer(
    model=model_clm,
    args=training_args_clm,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer
)

trainer_clm.train()

import os

drive_path = "/content/drive/MyDrive/LLaMA3_70B_finetune"  # Change this if needed
os.makedirs(drive_path, exist_ok=True)

"""
model.save_pretrained(drive_path)
tokenizer.save_pretrained(drive_path)

print(f"âœ… Model saved to {drive_path}")


from transformers import LlamaForCausalLM, AutoTokenizer

model = LlamaForCausalLM.from_pretrained(drive_path, torch_dtype=torch.float16, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(drive_path)

print("âœ… Model loaded from Google Drive!")
"""

lora_path = os.path.join(drive_path, "LoRA_adapters")
model.save_pretrained(lora_path)
print(f"âœ… LoRA adapters saved to {lora_path}")

from torch.utils.data import DataLoader
from transformers import LlamaForSequenceClassification

model_cls = LlamaForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 labels: positive, neutral, negative

train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=8, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["test"], batch_size=8)

training_args_cls = TrainingArguments(
    output_dir="./llama_norwegian_cls",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=5
)

trainer_cls = Trainer(
    model=model_cls,
    args=training_args_cls,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"]
)

trainer_cls.train()

